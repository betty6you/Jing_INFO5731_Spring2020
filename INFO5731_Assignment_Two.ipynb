{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/betty6you/Jing_INFO5731_Spring2020/blob/main/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "\n",
        "\n",
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#CovidVaccine\"](https://twitter.com/hashtag/CovidVaccine) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "c23eaf13-2042-4361-dbad-f0e0ade13218"
      },
      "source": [
        "# Write your code here\n",
        "# I will collect the all the customer reviews of the product 2019 Dell labtop on Amazon\n",
        "#!pip install Scrapy\n",
        "\n",
        "#!scrapy startproject AmazonReviewScraping\n",
        "#!scrapy genspider DellReviews https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/product-reviews/B07N49F51N/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class AmazonReviewsSpider(scrapy.Spider):\n",
        "\n",
        "  name = \"reviews\"\n",
        "  allowed_domains=['amazon.com']\n",
        "  myBaseUrl=\"https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/product-reviews/B07N49F51N/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=\"\n",
        "  startURL=[]\n",
        "  for i in range (1,13):\n",
        "    startURL.append(myBaseUrl+str(i))\n",
        "    \n",
        "  def parse(self, response):\n",
        "    #Get the Review List\n",
        "    data = response.css('#cm_cr-review_list')\n",
        "    \n",
        "    #Get the Name\n",
        "    name = data.css('.a-profile-name')\n",
        "            \n",
        "    #Get the Review Title\n",
        "    title = data.css('.review-title')\n",
        "            \n",
        "    # Get the Ratings\n",
        "    star_rating = data.css('.review-rating')\n",
        "    # Get the users Comments\n",
        "    comments = data.css('.review-text')\n",
        "    count = 0\n",
        "           \n",
        "    # combining the results\n",
        "    for review in star_rating:\n",
        "      yield{\n",
        "      'Rating': ''.join(review.xpath('.//text()').extract()),\n",
        "      'Comment': ''.join(comments[count].xpath(\".//text()\").extract())\n",
        "         }\n",
        "      count=count+1\n",
        "\n",
        "# run file in terminal !scrapy runspider AmazonReviewScraping/AmazonReviewScraping/spiders/DellSpider.py -o reviews.csv    \n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-23 22:51:14 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
            "2021-02-23 22:51:14 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.10 (default, Feb 20 2021, 21:17:23) - [GCC 7.5.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1j  16 Feb 2021), cryptography 3.4.6, Platform Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2021-02-23 22:51:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2021-02-23 22:51:15 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'SPIDER_LOADER_WARN_ONLY': True}\n",
            "2021-02-23 22:51:15 [scrapy.extensions.telnet] INFO: Telnet Password: 7874a178458d9af3\n",
            "2021-02-23 22:51:15 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2021-02-23 22:51:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2021-02-23 22:51:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2021-02-23 22:51:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2021-02-23 22:51:15 [scrapy.core.engine] INFO: Spider opened\n",
            "2021-02-23 22:51:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2021-02-23 22:51:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2021-02-23 22:51:15 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2021-02-23 22:51:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'elapsed_time_seconds': 0.005204,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2021, 2, 23, 22, 51, 15, 205482),\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 119103488,\n",
            " 'memusage/startup': 119103488,\n",
            " 'start_time': datetime.datetime(2021, 2, 23, 22, 51, 15, 200278)}\n",
            "2021-02-23 22:51:15 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9035e1d4-607b-472c-be2b-4b258dbfe71a"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop=stopwords.words('english')\n",
        "\n",
        "data=pd.read_csv(r\"https://raw.githubusercontent.com/betty6you/Jing_INFO5731_Spring2020/main/Review.csv\")\n",
        "data['comment'] = data['comment'].apply(lambda x: x.replace(r\"'\", ''))\n",
        "data['comment'] = data['comment'].apply(lambda x: x.replace(r'\"', ''))\n",
        "data['comment'] = data['comment'].apply(lambda x: x.replace(r'[\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    , \\n  ', ''))\n",
        "data['comment'] = data['comment'].apply(lambda x: x.replace(r'\\n, \\n  \\n]', ''))\n",
        "\n",
        "data['stopwords']=data['comment'].apply(lambda x:len ([x for x in x.split( ) if x in stop])) #remove stopwords\n",
        "data['special_char']=data['comment'].apply(lambda x: len([x for x in x.split() if x.startswith(('**','Â','â€','©','€™'))])) #Remove sepcial characters as noise\n",
        "\n",
        "data['commnet'] = data['comment'].str.rstrip(string.digits)#remove numbers\n",
        "data['comment']=data['comment'].str.replace('[^\\w\\s]','') # punctuation removal\n",
        "\n",
        "data['comment']=data['comment'].apply(lambda x:\" \".join(x.lower() for x in x.split()))# lower casing\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "data['comment'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()])) # Stemming\n",
        "\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "data['comment']= data['comment'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) # Lemmatization\n",
        "\n",
        "data['comment'].to_csv(\"AmazonReviewupdated.csv\",header=None,index=False) # only 1 column with clean sentences"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZsCfsbS9y3L"
      },
      "source": [
        "# Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnPc6rB6Ejy0",
        "outputId": "583df576-ffb0-4bc5-8d9e-a7cb12eb269e"
      },
      "source": [
        "'''\n",
        "Write your explanations of the constituency parsing tree and dependency parsing tree here\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWrite your explanations of the constituency parsing tree and dependency parsing tree here\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}