{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-05.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/betty6you/Jing_INFO5731_Spring2020/blob/main/In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nqZepinOdBj"
      },
      "source": [
        "## The fifth In-class-exercise (2/23/2021, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpHLXEmUOdBk"
      },
      "source": [
        "In exercise-03, I asked you to collected 500 textual data based on your own information needs (If you didn't collect the textual data, you should recollect for this exercise). Now we need to think about how to represent the textual data for text classification. In this exercise, you are required to select 10 types of features (10 types of features but absolutely more than 10 features) in the followings feature list, then represent the 500 texts with these features. The output should be in the following format:\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The feature list:\n",
        "\n",
        "* (1) tf-idf features\n",
        "* (2) POS-tag features: number of adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (select some of them if you use pos-tag features)\n",
        "* (3) Linguistic features:\n",
        "  * number of right-branching nodes across all constituent types\n",
        "  * number of right-branching nodes for NPs only\n",
        "  * number of left-branching nodes across all constituent types\n",
        "  * number of left-branching nodes for NPs only\n",
        "  * number of premodifiers across all constituent types\n",
        "  * number of premodifiers within NPs only\n",
        "  * number of postmodifiers across all constituent types\n",
        "  * number of postmodifiers within NPs only\n",
        "  * branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes\n",
        "  * branching index for NPs only\n",
        "  * branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories\n",
        "  * branching weight index for NPs only \n",
        "  * modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories\n",
        "  * modification index for NPs only\n",
        "  * modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories\n",
        "  * modification weight index for NPs only\n",
        "  * coordination balance, i.e. the maximal length difference in coordinated constituents\n",
        "  \n",
        "  * density (density can be calculated using the ratio of folowing function words to content words) of determiners/quantifiers\n",
        "  * density of pronouns\n",
        "  * density of prepositions\n",
        "  * density of punctuation marks, specifically commas and semicolons\n",
        "  * density of auxiliary verbs\n",
        "  * density of conjunctions\n",
        "  * density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns\n",
        "  \n",
        "  * maximal and average NP length\n",
        "  * maximal and average AJP length\n",
        "  * maximal and average PP length\n",
        "  * maximal and average AVP length\n",
        "  * sentence length\n",
        "\n",
        "* Other features in your mind (ie., pre-defined patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "H1hXEZljOdBl",
        "outputId": "d3690300-3cf1-425e-c560-9a419b3877fa"
      },
      "source": [
        "import nltk\r\n",
        "import pandas as pd\r\n",
        "from nltk import pos_tag\r\n",
        "from nltk.corpus import stopwords\r\n",
        "#POS-tag features\r\n",
        "nltk.download('stopwords')\r\n",
        "stop=stopwords.words('english')\r\n",
        "text=pd.read_csv(r\"https://raw.githubusercontent.com/betty6you/Jing_INFO5731_Spring2020/main/Ex3.csv\")\r\n",
        "\r\n",
        "text['Title']=text['Title'].apply(lambda x:\" \".join(x.lower() for x in x.split()))#lower case\r\n",
        "\r\n",
        "text['Title']=text['Title'].str.replace('[^\\w\\s]','') # remove punctuation\r\n",
        "#print(text['Title'])\r\n",
        "text['stopwords']=text['Title'].apply(lambda x: [item for item in x.split() if item not in stop])\r\n",
        "#text['stopwords']=text['Title'].apply(lambda x: [x for x in x.split( ) if x in stop])# count stopwords number\r\n",
        "#print(text['stopwords'])\r\n",
        "\r\n",
        "text['POS-tag']=text['stopwords'].map(pos_tag)\r\n",
        "#print(text['POS-tag'].head())\r\n",
        "def count_tags(title_with_tags):\r\n",
        "    tag_count = {}\r\n",
        "    for word, tag in title_with_tags:\r\n",
        "        if tag in tag_count:\r\n",
        "            tag_count[tag] += 1\r\n",
        "        else:\r\n",
        "            tag_count[tag] = 1\r\n",
        "    return(tag_count)\r\n",
        "text['Count_tag']=text['POS-tag'].map(count_tags)\r\n",
        "text['Count_tag'].head()\r\n",
        "vocabulary = {}\r\n",
        "for row in text['POS-tag']:\r\n",
        "    for word, tag in row:\r\n",
        "        if word in vocabulary:\r\n",
        "            if tag in vocabulary[word]:\r\n",
        "                vocabulary[word][tag] += 1\r\n",
        "            else:\r\n",
        "                vocabulary[word][tag] = 1\r\n",
        "        else:\r\n",
        "            vocabulary[word] = {tag: 1}\r\n",
        "vocabulary_df = pd.DataFrame.from_dict(vocabulary, orient='index')\r\n",
        "vocabulary_df.fillna(value=0, inplace=True)\r\n",
        "tag = 'NN' # NN: noun, singular \r\n",
        "vocabulary_df.sort_values(by=tag, ascending=False).head(10) # top 10 words for a given tag"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NNS</th>\n",
              "      <th>VBP</th>\n",
              "      <th>JJ</th>\n",
              "      <th>NN</th>\n",
              "      <th>VBG</th>\n",
              "      <th>IN</th>\n",
              "      <th>VBZ</th>\n",
              "      <th>RB</th>\n",
              "      <th>VBD</th>\n",
              "      <th>VBN</th>\n",
              "      <th>CD</th>\n",
              "      <th>PRP</th>\n",
              "      <th>MD</th>\n",
              "      <th>VB</th>\n",
              "      <th>NNP</th>\n",
              "      <th>FW</th>\n",
              "      <th>DT</th>\n",
              "      <th>JJS</th>\n",
              "      <th>RP</th>\n",
              "      <th>RBR</th>\n",
              "      <th>JJR</th>\n",
              "      <th>RBS</th>\n",
              "      <th>CC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>epilepsy</th>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>study</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>treatment</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lobe</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>effect</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gene</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>association</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expression</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medicine</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seizure</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             NNS   VBP    JJ     NN  VBG   IN  ...  JJS   RP  RBR  JJR  RBS   CC\n",
              "epilepsy     0.0  52.0  19.0  226.0  0.0  4.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "study        0.0   0.0   0.0   47.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "treatment    0.0   0.0   0.0   38.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "lobe         0.0   0.0   0.0   37.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "effect       0.0   0.0   0.0   36.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "gene         0.0   0.0   0.0   25.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "association  0.0   0.0   0.0   24.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "expression   0.0   1.0   0.0   23.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "medicine     0.0   1.0   0.0   23.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "seizure      0.0   1.0   0.0   22.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[10 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "HElGXaVvGh0v",
        "outputId": "11618918-c1f4-4f54-ffb5-752b68918cab"
      },
      "source": [
        "import nltk\r\n",
        "import pandas as pd\r\n",
        "from nltk import pos_tag\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "stop=stopwords.words('english')\r\n",
        "text=pd.read_csv(r\"https://raw.githubusercontent.com/betty6you/Jing_INFO5731_Spring2020/main/Ex3.csv\")\r\n",
        "\r\n",
        "text['Title']=text['Title'].apply(lambda x:\" \".join(x.lower() for x in x.split()))#lower case\r\n",
        "\r\n",
        "text['Title']=text['Title'].str.replace('[^\\w\\s]','') # remove punctuation\r\n",
        "\r\n",
        "text['stopwords']=text['Title'].apply(lambda x: [item for item in x.split() if item not in stop])\r\n",
        "print(text['stopwords'])\r\n",
        "# tf-idf features\r\n",
        "\r\n",
        "tf1= (text['stopwords']).apply(lambda x: pd.value_counts(x)).sum(axis=0).reset_index()\r\n",
        "tf1.columns=['words','tf']\r\n",
        "tf1.head(10)\r\n",
        "import numpy as np\r\n",
        "for i,word in enumerate(tf1['words']):\r\n",
        "  tf1.loc[i,'idf']=np.log(text.shape[0]/(len(text[text['Title'].str.contains(word)])))\r\n",
        "tf1['tfidf']=tf1['tf']*tf1['idf']\r\n",
        "tf1\r\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "0      [roles, mechanisms, traditional, chinese, medi...\n",
            "1      [withdrawn, traditional, chinese, medicine, ep...\n",
            "2      [excavating, anticonvulsant, compounds, prescr...\n",
            "3             [traditional, chinese, medicine, epilepsy]\n",
            "4                           [progress, epilepsy, stroke]\n",
            "                             ...                        \n",
            "495    [mechanism, acupuncture, suppressing, epilepti...\n",
            "496    [adenosine, inhibits, spontaneous, glutamate, ...\n",
            "497    [chemical, kindling, induced, microinjection, ...\n",
            "498    [epileptic, model, acute, motor, pattern, indu...\n",
            "499    [radix, bupleuri, added, os, draconis, concha,...\n",
            "Name: stopwords, Length: 500, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>epilepsy</td>\n",
              "      <td>313.0</td>\n",
              "      <td>0.468405</td>\n",
              "      <td>146.610736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>roles</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.521461</td>\n",
              "      <td>11.042922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chinese</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.703749</td>\n",
              "      <td>155.041122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>active</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.828314</td>\n",
              "      <td>9.656627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>treating</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.115996</td>\n",
              "      <td>15.347987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1839</th>\n",
              "      <td>os</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.639897</td>\n",
              "      <td>1.639897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1840</th>\n",
              "      <td>draconis</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.214608</td>\n",
              "      <td>6.214608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1841</th>\n",
              "      <td>ostreae</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.214608</td>\n",
              "      <td>6.214608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1842</th>\n",
              "      <td>bupleuri</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.214608</td>\n",
              "      <td>6.214608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>neuropsychopathiesa</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.214608</td>\n",
              "      <td>6.214608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1844 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    words     tf       idf       tfidf\n",
              "0                epilepsy  313.0  0.468405  146.610736\n",
              "1                   roles    2.0  5.521461   11.042922\n",
              "2                 chinese   91.0  1.703749  155.041122\n",
              "3                  active    2.0  4.828314    9.656627\n",
              "4                treating    3.0  5.115996   15.347987\n",
              "...                   ...    ...       ...         ...\n",
              "1839                   os    1.0  1.639897    1.639897\n",
              "1840             draconis    1.0  6.214608    6.214608\n",
              "1841              ostreae    1.0  6.214608    6.214608\n",
              "1842             bupleuri    1.0  6.214608    6.214608\n",
              "1843  neuropsychopathiesa    1.0  6.214608    6.214608\n",
              "\n",
              "[1844 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7ybPHpHvzpd"
      },
      "source": [
        "# linguistic feature\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjDEpIJdhU_S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}